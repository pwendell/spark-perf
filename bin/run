#!/usr/bin/env python

import argparse
import itertools
from math import sqrt
import os.path
import re
from subprocess import Popen, PIPE
import sys
import threading
import time

# The perf-tests project directory.
proj_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sbt_cmd = "sbt/sbt"

parser = argparse.ArgumentParser(description='Run Spark or Shark peformance tests. Before running, '
    'edit the supplied configuration file.')

parser.add_argument('--config-file', help='override default location of config file, must be a '
    'python file that ends in .py', default="%s/config/config.py" % proj_dir)

args = parser.parse_args()
assert args.config_file.endswith(".py"), "config filename must end with .py"

# Check if the config file exists.
assert os.path.isfile(args.config_file), ("Please create a config file called %s (you probably "
    "just want to copy and then modify %s/config/config.py.template)" %
    (args.config_file, proj_dir))

print "Detected project directory: %s" % proj_dir
# Import the configuration settings from the config file.
print("Adding %s to sys.path" % os.path.dirname(args.config_file))
sys.path.append(os.path.dirname(args.config_file))
print("running 'import %s'" % os.path.basename(args.config_file).split(".")[0])
exec("import %s" % os.path.basename(args.config_file).split(".")[0])

# Determine which projects and tests to build based on user-specified tests and boolean flags from
# the config file.

# Do each project have tests
has_spark_tests = (len(config.SPARK_TESTS) > 0) and not config.SPARK_SKIP_TESTS
has_streaming_tests = (len(config.STREAMING_TESTS) > 0) and not config.STREAMING_SKIP_TESTS
has_shark_tests = (len(config.SHARK_TESTS) > 0) and not config.SHARK_SKIP_TESTS

# Should prep each project 
should_prep_spark = not config.SPARK_SKIP_PREP   # all project depend on Spark, so prepare Spark 
should_prep_shark = has_shark_tests and not config.SHARK_SKIP_PREP  # prepare Shark if it has tests 
should_prep_hive = should_prep_shark and not config.HIVE_SKIP_PREP  # prepare Hive if Shark needs to be prepared 

# Need jar for each project's tests. This captures the dependencies between different perf tests 
# (e.g. Shark tests depends on Spark tests)
need_spark_tests_jar = has_spark_tests or has_shark_tests # Shark tests require Spark tests jar
need_streaming_tests_jar = has_streaming_tests
need_shark_tests_jar = has_shark_tests

# Only build the perf test sources that will be used.
should_prep_spark_tests = need_spark_tests_jar and not config.SPARK_SKIP_TEST_PREP
should_prep_streaming_tests = need_streaming_tests_jar and not config.STREAMING_SKIP_TEST_PREP
should_prep_shark_tests = need_shark_tests_jar and not config.SHARK_SKIP_TEST_PREP

# Do disk warmup only if there are tests to run.
should_warmup_disk = (has_shark_tests or has_spark_tests) and not config.SKIP_DISK_WARMUP

# Check that commit ID's are specified in config_file.
if should_prep_spark:
    assert config.SPARK_COMMIT_ID is not "", \
        ("Please specify SPARK_COMMIT_ID in %s" % args.config_file)
if should_prep_shark:
    assert config.SHARK_COMMIT_ID is not "", \
        ("Please specifiy SHARK_COMMIT_ID in %s" % args.config_file)

# Run shell command and ignore output.
def run_cmd(cmd, exit_on_fail=True):
    if cmd.find(";") != -1:
        print("***************************")
        print("WARNING: the following command contains a semicolon which may cause non-zero return "
            "values to be ignored. This isn't necessarily a problem, but proceed with caution!")
    print(cmd)
    return_code = Popen(cmd, stdout=sys.stderr, shell=True).wait()
    if exit_on_fail:
        if return_code != 0:
            print "The following shell command finished with a non-zero returncode (%s): %s" % (
                return_code, cmd)
            sys.exit(-1)
    return return_code

# Run several commands in parallel, waiting for them all to finish.
# Expects an array of tuples, where each tuple consists of (command_name, exit_on_fail).
def run_cmds_parallel(commands):
    threads = []
    for (cmd_name, exit_on_fail) in commands:
        thread = threading.Thread(target=run_cmd, args=(cmd_name, exit_on_fail))
        thread.start()
        threads = threads + [thread]
    for thread in threads:
        thread.join()

# Return a command running cmd_name on host with proper SSH configs.
def make_ssh_cmd(cmd_name, host):
    return "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 %s '%s'" % (host, cmd_name)

# Return a command which copies the supplied directory to the given host.
def make_rsync_cmd(dir_name, host):
    return ('rsync --delete -e "ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5" -az "%s/" '
        '"%s:%s"') % (dir_name, host, os.path.abspath(dir_name))

# Delete all files in the given directory on the specified hosts.
def clear_dir(dir_name, hosts):
    assert dir_name != "" and dir_name != "/", ("Attempted to delete directory '%s/*', halting "
        "rather than deleting entire file system.") % dir_name
    if config.PROMPT_FOR_DELETES:
        response = raw_input("\nAbout to remove all files and directories under %s on %s, is "
            "this ok? [y, n] " % (dir_name, hosts))
        if response != "y":
            return
    run_cmds_parallel([(make_ssh_cmd("rm -r %s/*" % dir_name, host), False) for host in hosts])

# Ensures that no executors are running on Spark slaves. Executors can continue to run for some
# time after a shutdown signal is given due to cleaning up temporary files.
def ensure_spark_stopped_on_slaves(slaves):
    stop = False
    while not stop:
        cmd = "ps -ef | grep -v grep | grep ExecutorBackend"
        ret_vals = map(lambda s: run_cmd(make_ssh_cmd(cmd, s), False), slaves)
        if 0 in ret_vals:
            print "Spark is still running on some slaves ... sleeping for 10 seconds"
            time.sleep(10)
        else:
            stop = True

# Get a list of slaves by parsing the slaves file in SPARK_CONF_DIR.
slaves_file_raw = open("%s/slaves" % config.SPARK_CONF_DIR, 'r').read().split("\n")
slaves_list = filter(lambda x: not x.startswith("#") and not x is "", slaves_file_raw)

# If a cluster is already running from the Spark EC2 scripts, try shutting it down.
if os.path.exists("%s/bin/stop-all.sh" % config.SPARK_HOME_DIR):
    run_cmd("%s/bin/stop-all.sh" % config.SPARK_HOME_DIR)

# If a cluster is already running from an earlier test, try shutting it down.
if os.path.exists("%s/spark/bin/stop-all.sh" % proj_dir):
    print("Stopping Spark standalone cluster...")
    run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)

# Ensure all shutdowns have completed (no executors are running).
ensure_spark_stopped_on_slaves(slaves_list)
# Allow some extra time for slaves to fully terminate.
time.sleep(5)

# Prepare Spark.
if should_prep_spark:
    # Assumes that the preexisting 'spark' directory is valid.
    if not os.path.isdir("spark"):
        # Clone Spark.
        print("Git cloning Spark...")
        run_cmd("git clone %s spark" % config.SPARK_GIT_REPO)
        run_cmd("cd spark; git config --add remote.origin.fetch "
            "'+refs/pull/*/head:refs/remotes/origin/pr/*'")
        run_cmd("cd spark; git config --add remote.origin.fetch "
            "'+refs/tags/*:refs/remotes/origin/tag/*'")

    # Fetch updates.
    os.chdir("spark")
    print("Updating Spark repo...")
    run_cmd("git fetch")

    # Build Spark.
    print("Cleaning Spark and building branch %s. This may take a while...\n" %
        config.SPARK_COMMIT_ID)
    run_cmd("git clean -f -d -x")

    if config.SPARK_MERGE_COMMIT_INTO_MASTER:
        run_cmd("git reset --hard master")
        run_cmd("git merge %s -m ='Merging %s into master.'" %
            (config.SPARK_COMMIT_ID, config.SPARK_COMMIT_ID))
    else:
        run_cmd("git reset --hard %s" % config.SPARK_COMMIT_ID)

    run_cmd("%s clean assembly/assembly" % sbt_cmd)

    # Copy Spark configuration files to new directory.
    print("Copying all files from %s to %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))
    assert os.path.exists("%s/spark-env.sh" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/spark-env.sh" % config.SPARK_CONF_DIR
    assert os.path.exists("%s/slaves" % config.SPARK_CONF_DIR), \
        "Could not find required file %s/slaves" % config.SPARK_CONF_DIR
    run_cmd("cp %s/* %s/spark/conf/" % (config.SPARK_CONF_DIR, proj_dir))

    # Change back to 'proj_dir' directory.
    os.chdir("..")
else:
    # TODO(andy): Make this check for the jar we will be including on the
    #             classpath (as part of work Patrick is doing), instead of
    #             just looking for spark/target.
    assert os.path.exists(
        "%s/spark/core/target" % proj_dir), ("You chose to skip Spark prep, but we can't since " +
        "%s/spark/target does not exist, so Spark needs to be rebuilt/packaged." % proj_dir)
    print("Skipping preparation tasks for Spark (i.e, git clone or update, checkout, build, " +
        "copy conf files).")

# Prepare Hive.
if should_prep_hive:
    # Assumes that the preexisting 'hive' directory is valid.
    if not os.path.isdir("hive"):
        # Clone the Hive repository.
        print("Git cloning Hive...")
        run_cmd("git clone %s hive" % config.HIVE_GIT_REPO)

    # Build Hive.
    os.chdir("hive")
    run_cmd("ant clean package")

    # Change back to 'proj_dir' directory.
    os.chdir("..")
else:
    if has_shark_tests:
        assert os.path.exists(
            "%s/hive/build/dist/lib" % proj_dir), ("You chose to skip Hive prep, but we can't " +
            "since %s/hive/build/dist/lib does not exist, so Hive needs to be rebuilt/packaged." %
            proj_dir)
    print("Skipping preparation tasks for Hive (i.e, git clone, build).")

# Prepare Shark.
if should_prep_shark:
    if not os.path.isdir("shark"):
        # Clone the Shark repository.
        print("Git cloning Shark...")
        run_cmd("git clone %s shark" % config.SHARK_GIT_REPO)
        run_cmd("cd shark; git config --add remote.origin.fetch "
            "'+refs/pull/*/head:refs/remotes/origin/pr/*'")
        run_cmd("cd shark; git config --add remote.origin.fetch "
            "'+refs/tags/*:refs/remotes/origin/tag/*'")

    # Fetch updates.
    os.chdir("shark")
    print("Updating Shark repo...")
    run_cmd("git fetch")

    # Build Shark.
    print("Cloning Shark and building branch %s. This may take a while...\n" %
        config.SHARK_COMMIT_ID)
    run_cmd("git clean -f -d -x")

    if config.SHARK_MERGE_COMMIT_INTO_MASTER:
        run_cmd("git reset --hard master")
        run_cmd("git merge %s -m ='Merging %s into master.'" %
            config.SHARK_COMMIT_ID, config.SHARK_COMMIT_ID)
    else:
        run_cmd("git reset --hard %s" % config.SHARK_COMMIT_ID)

    run_cmd("%s clean package" % sbt_cmd)

    # Copy Shark configuration files from the SHARK_CONF_DIR specified in config.py.
    print("Copying all files from %s to %s/shark/conf/" % (config.SHARK_CONF_DIR, proj_dir))
    assert os.path.exists("%s/shark-env.sh" % config.SHARK_CONF_DIR), \
        "Could not find required file %s/shark-env.sh" % config.SHARK_CONF_DIR
    run_cmd("cp %s/* %s/shark/conf/" % (config.SHARK_CONF_DIR, proj_dir))

    # Change from 'perf-tests/shark' back to the 'proj_dir' directory.
    os.chdir("..")
else:
    if has_shark_tests:
        assert os.path.exists(
            "%s/shark/target" % proj_dir), ("You chose to skip Shark prep, but we can't since "
            "%s/shark/target does not exist, so Shark needs to be rebuilt/packaged." % proj_dir)
    print("Skipping preparation tasks for Shark (i.e, git clone or update, checkout, build, copy).")

# Build the tests for each project based on copies cloned to the 'proj_dir'.
spark_work_dir = "%s/spark/work" % proj_dir
print("Trying to delete spark work dir " + spark_work_dir)
if os.path.exists(spark_work_dir):
    println("Actually deleting.")
    # Clear the 'perf-tests/spark/work' directory beforehand, since that may contain scheduler logs
    # from previous jobs. The directory could also contain a spark-perf-tests-assembly.jar that may
    # interfere with subsequent 'sbt assembly' for Spark perf.
    clear_dir(spark_work_dir, ["localhost"])
else:
    print("Spark work path " + spark_work_dir + " no found.")

if need_spark_tests_jar:
    print("Building Spark tests...")
    if should_prep_spark_tests:
        run_cmd("cd %s/spark-tests; %s clean assembly" % (proj_dir, sbt_cmd))
    else:
        spark_test_jar_path = "%s/spark-tests/target/spark-perf-tests-assembly.jar" % proj_dir
        assert os.path.exists(spark_test_jar_path), ("You tried to skip packaging the Spark perf " +
            "tests, but %s was not already present") % spark_test_jar_path

if need_streaming_tests_jar:
    print("Building Spark Streaming tests...")
    if should_prep_streaming_tests:
        run_cmd("cd %s/streaming-tests; %s clean assembly" % (proj_dir, sbt_cmd))
    else:
        streaming_test_jar_path = "%s/streaming-tests/target/streaming-perf-tests-assembly.jar" % proj_dir
        assert os.path.exists(streaming_test_jar_path), ("You tried to skip packaging the Spark Streaming perf " +
            "tests, but %s was not already present") % spark_test_jar_path

if need_shark_tests_jar:
    print("Building Shark tests...")
    if should_prep_shark_tests:
        run_cmd("cd %s/shark-tests; %s clean assembly" % (proj_dir, sbt_cmd))
    else:
        shark_test_jar_path = "%s/shark-tests/target/shark-perf-tests-assembly.jar" % proj_dir
        assert os.path.exists(shark_test_jar_path), ("You tried to skip packaging the Shark perf " +
            "tests, but %s was not already present") % shark_test_jar_path

# Sync the whole directory to the slaves. 
print("Syncing the test directory to the slaves.")
if len(slaves_list) > 1 or slaves_list[0] != "localhost" : # no need to sync if running locally
    run_cmds_parallel([(make_rsync_cmd(proj_dir, slave), True) for slave in slaves_list])

# Start our Spark cluster.
print("Starting a Spark standalone cluster to use for testing...")
assert os.path.exists("%s/spark/bin/start-all.sh" % proj_dir), ("%s/spark/bin/start-all.sh must "
    "exist") % proj_dir
run_cmd("%s/spark/bin/start-all.sh" % proj_dir)
time.sleep(5) # Starting the cluster takes a little time so give it a second.

# Set Spark Java Options (from config.py)
new_env = os.environ.copy()

new_env["SPARK_HOME"] = "%s/spark" % proj_dir

# Search for 'spark.local.dir' in spark-env.sh, or shark-env.sh if Shark is also being built. Note
# that for the latter case, 'spark.local.dir' in shark-env.sh takes precedence.
spark_local_dirs = ""
path_to_env_file = ""
if has_shark_tests:
    # Store Hive metastore and warehouse subdirectories created by each Shark PerfTest.
    perf_test_warehouses_dir = "%s/test-warehouses" % proj_dir
    new_env["PERF_TEST_WAREHOUSES"] = perf_test_warehouses_dir
    path_to_env_file = "%s/shark-env.sh" % config.SHARK_CONF_DIR
else:
    path_to_env_file = "%s/spark-env.sh" % config.SPARK_CONF_DIR

env_file_content = open(path_to_env_file, 'r').read()
re_result = re.search(r'spark.local.dir=([^"]*)"', env_file_content)
if re_result:
    spark_local_dirs = re_result.group(1).split(",")
else:
    sys.exit("ERROR: These scripts require you to explicitly set spark.local.dir in spark-env.sh "
        "(or shark-env.sh) so that it can be cleaned. The way we check this is pretty picky, "
        "specifically we try to find the following string in spark-env.sh (or shark-env.sh): "
        "spark.local.dir=ONE_OR_MORE_DIRNAMES\" so you will want a line like this: "
        "SPARK_JAVA_OPTS+=\" -Dspark.local.dir=/tmp\"")

if should_warmup_disk:
    for local_dir in spark_local_dirs:
        # Strip off any trailing whitespace(s) so that the clear commands below can work properly.
        local_dir = local_dir.rstrip()

        bytes_to_write = config.DISK_WARMUP_BYTES
        bytes_per_file = bytes_to_write / config.DISK_WARMUP_FILES
        gen_command = "dd if=/dev/urandom bs=%s count=1 | split -a 5 -b %s - %s/random" % (
            bytes_to_write, bytes_per_file, local_dir)
        # Ensures the directory exists.
        dir_command = "mkdir -p %s" % local_dir
        clear_command = "rm -f %s/*" % local_dir

        print("Generating test data for %s, this may take some time" % local_dir)
        all_hosts = slaves_list + ["localhost"]
        run_cmds_parallel([(make_ssh_cmd(dir_command, host), True) for host in all_hosts])
        run_cmds_parallel([(make_ssh_cmd(gen_command, host), True) for host in all_hosts])
        clear_dir(local_dir, all_hosts)

# Some utility functions to calculate useful stats on test output.
def average(in_list):
    return sum(in_list) / len(in_list)

def variance(in_list):
    variance = 0
    for x in in_list:
        variance = variance + (average(in_list) - x) ** 2
    return variance / len(in_list)

# Append configuration of a test run to a log file
def append_config_to_file(filename, java_opt_list, opt_list):
    output_divider_string = "--------------------------------------------------------------------"
    with open (filename, "a") as fout:
        fout.write(output_divider_string + "\n")
        fout.write("Java options: %s\n" % (" ".join(java_opt_list)))
        fout.write("Options: %s\n" % (" ".join(opt_list)))
        fout.write(output_divider_string + "\n")
        fout.flush()
    

# Run all tests specified in 'tests_to_run', a list of 5-element tuples. See the 'Test Setup'
# section in config.py.template for more info.
# Results are written to 'output_filename'.
#
# TODO: (written by TD) 
# This is an intermediate solution that takes the function to process the output of the tests as a function.
# This is done because Spark/Shark tests and Streaming tests process the output differently. Ideal solution, is
# that each test will do whatever processing necessary for it (e.g. generating stats on multiple trials) and return a 
# single string as a result that will written to the output file. 

def run_tests(scala_cmd_classpath, tests_to_run, test_group_name, output_processing_function, output_filename):
    output_dirname = output_filename + "_logs"
    os.makedirs(output_dirname)
    out_file = open(output_filename, 'w')
    num_tests_to_run = len(tests_to_run)

    output_divider_string = "--------------------------------------------------------------------"
    print(output_divider_string)
    print("Running %d tests in %s.\n" % (num_tests_to_run, test_group_name))

    for short_name, test_cmd, scale_factor, java_opt_sets, opt_sets in tests_to_run:
        print(output_divider_string)
        print("Running test command: '%s' ..." % test_cmd)
        stdout_filename = "%s/%s.out" % (output_dirname, short_name) 
        stderr_filename = "%s/%s.err" % (output_dirname, short_name) 
        
        # Run a test for all combinations of the OptionSets given, then capture
        # and print the output.
        java_opt_set_arrays = [i.to_array(scale_factor) for i in java_opt_sets]
        opt_set_arrays = [i.to_array(scale_factor) for i in opt_sets]
        for java_opt_list in itertools.product(*java_opt_set_arrays):
            for opt_list in itertools.product(*opt_set_arrays):
                ensure_spark_stopped_on_slaves(slaves_list)
                append_config_to_file(stdout_filename, java_opt_list, opt_list)
                append_config_to_file(stderr_filename, java_opt_list, opt_list)
                cmd = "%s %s -cp %s %s %s %s 1>> %s 2>> %s" % (config.SCALA_CMD, " ".join(java_opt_list),
                    scala_cmd_classpath, test_cmd, config.SPARK_CLUSTER_URL, " ".join(opt_list), 
                    stdout_filename, stderr_filename)

                print("\nRunning command: %s\n" % cmd)
                Popen(cmd, shell=True, env=new_env).wait()
                result_string = output_processing_function(short_name, opt_list, stdout_filename, stderr_filename)
                print(output_divider_string)
                print("\nResult: " + result_string) 
                print(output_divider_string)
                out_file.write(result_string + "\n")
                out_file.flush()

    print("\nFinished running %d tests in %s.\nSee summary in %s" %
        (num_tests_to_run, test_group_name, output_filename))
    print(output_divider_string)


# Function to process the output of multiple trials for Spark and Shark tests 
def process_trials_output(short_name, opt_list, stdout_filename, stderr_filename):
    with open (stdout_filename, "r") as stdout_file:
        output = stdout_file.read()
    results_token = "results: "
    if results_token not in output:
        print("Test did not produce expected results. Output was:")
        print(output)
        sys.exit(1)
    result_line = filter(lambda x: results_token in x, output.split("\n"))[0]
    result_list = result_line.replace(results_token, "").split(",")
    assert len(result_list) > config.IGNORED_TRIALS, ("Expecting at least %s results "
        "but only found %s" % (config.IGNORED_TRIALS + 1, len(result_list)))
    result_list = result_list[config.IGNORED_TRIALS:]
    result_first = result_list[0]
    result_last = result_list[len(result_list) - 1]

    # TODO(andy): For even cardinality lists, return average of middle two elts.
    result_list = sorted([float(x) for x in result_list])
    result_med = result_list[len(result_list)/2]
    result_std = sqrt(variance(result_list))
    result_min = min(result_list)
    result_string = "%s, %s, %s, %.3f, %s, %s, %s" % (short_name, " ".join(opt_list),
        result_med, result_std, result_min, result_first, result_last)
    sys.stdout.flush()
    return result_string


# Function to process output of streaming tests
def process_streaming_output(short_name, opt_list, stdout_filename, stderr_filename):
    lastlines = Popen("tail -5 %s" % stdout_filename, shell=True, stdout=PIPE).stdout.read()
    # result_token = "PASSED"
    results_token = "Result: "
    if results_token not in lastlines:
        result = "FAILED"
    else:
        result = filter(lambda x: results_token in x, lastlines.split("\n"))[0].replace(results_token, "")
    result_string = "%s [ %s ] - %s" % (short_name, " ".join(opt_list), result)
    return str(result_string)


# Run all Spark and/or Shark tests specified in the Config file.
scala_cmd_classpath = "%s/spark-tests/target/spark-perf-tests-assembly.jar" % proj_dir
spark_test_classpath = "%s/spark-tests/target/spark-perf-tests-assembly.jar" % proj_dir
streaming_test_classpath = "%s/streaming-tests/target/streaming-perf-tests-assembly.jar" % proj_dir
shark_test_classpath  = "%s/shark-tests/target/shark-perf-tests-assembly.jar" % proj_dir

if has_spark_tests:
    scala_cmd_classpath = spark_test_classpath 
    run_tests(scala_cmd_classpath, config.SPARK_TESTS, "Spark-Tests",
        process_trials_output, config.SPARK_OUTPUT_FILENAME)

if has_streaming_tests:
    scala_cmd_classpath = streaming_test_classpath 
    run_tests(scala_cmd_classpath, config.STREAMING_TESTS, "Streaming-Tests",
        process_streaming_output, config.STREAMING_OUTPUT_FILENAME)

if has_shark_tests:
    # Set up the 'perf_test_warehouses'.
    if os.path.isdir(perf_test_warehouses_dir):
        clear_dir(perf_test_warehouses_dir, ["localhost"])
    else:
        os.mkdir(perf_test_warehouses_dir)
    scala_cmd_classpath = spark_test_classpath + ":" + shark_test_classpath 
    run_tests(scala_cmd_classpath, config.SHARK_TESTS, "Shark-Tests",
        process_trials_output, config.SHARK_OUTPUT_FILENAME)

print("All tests have finished running. Stopping Spark standalone cluster ...")
run_cmd("%s/spark/bin/stop-all.sh" % proj_dir)

print("Finished running all tests.")
